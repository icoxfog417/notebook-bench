{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"AOMxjtN9pPN8","executionInfo":{"status":"ok","timestamp":1669871843986,"user_tz":-540,"elapsed":1659,"user":{"displayName":"Takahiro Kubo","userId":"12892517346285462954"}},"outputId":"8afb2071-a1dd-46e2-d257-db10efe5c023","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'notebook-bench'...\n","remote: Enumerating objects: 45, done.\u001b[K\n","remote: Counting objects: 100% (45/45), done.\u001b[K\n","remote: Compressing objects: 100% (33/33), done.\u001b[K\n","remote: Total 45 (delta 5), reused 42 (delta 5), pack-reused 0\u001b[K\n","Unpacking objects: 100% (45/45), done.\n"]}],"source":["!git clone https://github.com/icoxfog417/notebook-bench.git"]},{"cell_type":"code","source":["%cd notebook-bench/"],"metadata":{"id":"3lFLP4OnpwAh","executionInfo":{"status":"ok","timestamp":1669871843987,"user_tz":-540,"elapsed":5,"user":{"displayName":"Takahiro Kubo","userId":"12892517346285462954"}},"outputId":"3bec72bf-ac0a-4269-d392-f9668ed12aa8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/notebook-bench\n"]}]},{"cell_type":"code","source":["%pip install -r requirements.txt"],"metadata":{"id":"2EIOE_EvplCL","executionInfo":{"status":"ok","timestamp":1669871882078,"user_tz":-540,"elapsed":38094,"user":{"displayName":"Takahiro Kubo","userId":"12892517346285462954"}},"outputId":"e7dfb55d-8735-490a-e10f-03e565676fe5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers[torch]==4.24.0\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 27.2 MB/s \n","\u001b[?25hCollecting fugashi==1.2.0\n","  Downloading fugashi-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615 kB)\n","\u001b[K     |████████████████████████████████| 615 kB 84.8 MB/s \n","\u001b[?25hCollecting ipadic==1.0.0\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[K     |████████████████████████████████| 13.4 MB 83.8 MB/s \n","\u001b[?25hCollecting pandas==1.5.2\n","  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n","\u001b[K     |████████████████████████████████| 12.2 MB 81.7 MB/s \n","\u001b[?25hCollecting scikit-learn==1.1.3\n","  Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n","\u001b[K     |████████████████████████████████| 31.2 MB 143.0 MB/s \n","\u001b[?25hCollecting datasets==2.7.1\n","  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 102.0 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.97\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 88.6 MB/s \n","\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (3.6.4)\n","Collecting pre-commit\n","  Downloading pre_commit-2.20.0-py2.py3-none-any.whl (199 kB)\n","\u001b[K     |████████████████████████████████| 199 kB 98.0 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.2->-r requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.2->-r requirements.txt (line 4)) (2022.6)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.2->-r requirements.txt (line 4)) (1.21.6)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.3->-r requirements.txt (line 5)) (1.7.3)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.3->-r requirements.txt (line 5)) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.1.3->-r requirements.txt (line 5)) (3.1.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (4.64.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (2022.11.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (6.0)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (0.3.6)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 91.2 MB/s \n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 95.6 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (3.8.3)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 100.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (21.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (2.23.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.7.1->-r requirements.txt (line 6)) (9.0.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.24.0->-r requirements.txt (line 1)) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 84.5 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.24.0->-r requirements.txt (line 1)) (3.8.0)\n","Requirement already satisfied: torch!=1.12.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.24.0->-r requirements.txt (line 1)) (1.12.1+cu113)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.1->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.1->-r requirements.txt (line 6)) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.1->-r requirements.txt (line 6)) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.1->-r requirements.txt (line 6)) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.1->-r requirements.txt (line 6)) (22.1.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.1->-r requirements.txt (line 6)) (2.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.7.1->-r requirements.txt (line 6)) (4.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.7.1->-r requirements.txt (line 6)) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets==2.7.1->-r requirements.txt (line 6)) (3.0.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.2->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.7.1->-r requirements.txt (line 6)) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.7.1->-r requirements.txt (line 6)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.7.1->-r requirements.txt (line 6)) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.7.1->-r requirements.txt (line 6)) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 96.4 MB/s \n","\u001b[?25hRequirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 8)) (9.0.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 8)) (1.11.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 8)) (57.4.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 8)) (1.4.1)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 8)) (0.7.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from pre-commit->-r requirements.txt (line 9)) (0.10.2)\n","Collecting identify>=1.0.0\n","  Downloading identify-2.5.9-py2.py3-none-any.whl (98 kB)\n","\u001b[K     |████████████████████████████████| 98 kB 11.8 MB/s \n","\u001b[?25hCollecting cfgv>=2.0.0\n","  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n","Collecting nodeenv>=0.11.1\n","  Downloading nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n","Collecting virtualenv>=20.0.8\n","  Downloading virtualenv-20.17.0-py3-none-any.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 85.6 MB/s \n","\u001b[?25hCollecting distlib<1,>=0.3.6\n","  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n","\u001b[K     |████████████████████████████████| 468 kB 96.9 MB/s \n","\u001b[?25hCollecting platformdirs<3,>=2.4\n","  Downloading platformdirs-2.5.4-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: ipadic\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=5519b97b2f6f1659f8d939d6993fea8fb41860544d600ddbb6686276ac31cc8c\n","  Stored in directory: /root/.cache/pip/wheels/45/b7/f5/a21e68db846eedcd00d69e37d60bab3f68eb20b1d99cdff652\n","Successfully built ipadic\n","Installing collected packages: urllib3, tokenizers, platformdirs, huggingface-hub, distlib, xxhash, virtualenv, transformers, responses, pandas, nodeenv, multiprocess, identify, cfgv, sentencepiece, scikit-learn, pre-commit, ipadic, fugashi, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","Successfully installed cfgv-3.3.1 datasets-2.7.1 distlib-0.3.6 fugashi-1.2.0 huggingface-hub-0.11.1 identify-2.5.9 ipadic-1.0.0 multiprocess-0.70.14 nodeenv-1.7.0 pandas-1.5.2 platformdirs-2.5.4 pre-commit-2.20.0 responses-0.18.0 scikit-learn-1.1.3 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0 urllib3-1.25.11 virtualenv-20.17.0 xxhash-3.1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"3E2vhayLpPOB"},"source":["### For Studio Lab\n","\n","Create the environment before runnning the notebok"]},{"cell_type":"markdown","metadata":{"id":"pinz9wCPpPOD"},"source":["## Calculation"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38F5TtVYHbe0","executionInfo":{"status":"ok","timestamp":1669871882673,"user_tz":-540,"elapsed":613,"user":{"displayName":"Takahiro Kubo","userId":"12892517346285462954"}},"outputId":"65eb8b09-ea8a-4fe7-e8a3-2814d6d918bc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Dec  1 05:18:01 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"w2C-rkIOpPOF","executionInfo":{"status":"ok","timestamp":1669871882674,"user_tz":-540,"elapsed":6,"user":{"displayName":"Takahiro Kubo","userId":"12892517346285462954"}}},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9360a510","outputId":"80d99736-17c5-477f-9e02-8d23712a1993","executionInfo":{"status":"ok","timestamp":1669873287622,"user_tz":-540,"elapsed":1404953,"user":{"displayName":"Takahiro Kubo","userId":"12892517346285462954"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-12-01 05:18:03.565397: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Downloading: 100% 479/479 [00:00<00:00, 469kB/s]\n","Downloading: 100% 445M/445M [00:04<00:00, 104MB/s]\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Downloading: 100% 110/110 [00:00<00:00, 96.8kB/s]\n","Downloading: 100% 258k/258k [00:00<00:00, 286kB/s]\n","Downloading builder script: 100% 7.16k/7.16k [00:00<00:00, 5.83MB/s]\n","Downloading metadata: 100% 37.4k/37.4k [00:00<00:00, 166kB/s] \n","Downloading readme: 100% 13.4k/13.4k [00:00<00:00, 59.4kB/s]\n","Downloading and preparing dataset amazon_reviews_multi/ja to /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609...\n","Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n","Downloading data:   0% 0.00/169M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   0% 18.4k/169M [00:00<25:14, 112kB/s]\u001b[A\n","Downloading data:   0% 52.2k/169M [00:00<19:18, 146kB/s]\u001b[A\n","Downloading data:   0% 122k/169M [00:00<11:15, 251kB/s] \u001b[A\n","Downloading data:   0% 261k/169M [00:00<06:02, 467kB/s]\u001b[A\n","Downloading data:   0% 557k/169M [00:00<03:10, 886kB/s]\u001b[A\n","Downloading data:   1% 1.15M/169M [00:01<01:39, 1.69MB/s]\u001b[A\n","Downloading data:   1% 2.31M/169M [00:01<00:52, 3.20MB/s]\u001b[A\n","Downloading data:   3% 4.64M/169M [00:01<00:26, 6.21MB/s]\u001b[A\n","Downloading data:   4% 6.93M/169M [00:01<00:19, 8.16MB/s]\u001b[A\n","Downloading data:   5% 9.31M/169M [00:01<00:16, 9.61MB/s]\u001b[A\n","Downloading data:   7% 11.9M/169M [00:02<00:14, 11.0MB/s]\u001b[A\n","Downloading data:   9% 14.6M/169M [00:02<00:12, 12.0MB/s]\u001b[A\n","Downloading data:  10% 17.2M/169M [00:02<00:12, 12.6MB/s]\u001b[A\n","Downloading data:  12% 19.8M/169M [00:02<00:11, 13.1MB/s]\u001b[A\n","Downloading data:  13% 22.5M/169M [00:02<00:10, 13.6MB/s]\u001b[A\n","Downloading data:  15% 25.1M/169M [00:02<00:10, 13.7MB/s]\u001b[A\n","Downloading data:  16% 27.7M/169M [00:03<00:10, 13.8MB/s]\u001b[A\n","Downloading data:  18% 30.3M/169M [00:03<00:10, 13.9MB/s]\u001b[A\n","Downloading data:  19% 32.9M/169M [00:03<00:09, 14.0MB/s]\u001b[A\n","Downloading data:  21% 35.6M/169M [00:03<00:09, 14.1MB/s]\u001b[A\n","Downloading data:  23% 38.3M/169M [00:03<00:09, 14.2MB/s]\u001b[A\n","Downloading data:  24% 40.9M/169M [00:04<00:09, 14.2MB/s]\u001b[A\n","Downloading data:  26% 43.6M/169M [00:04<00:08, 14.3MB/s]\u001b[A\n","Downloading data:  27% 46.3M/169M [00:04<00:08, 14.4MB/s]\u001b[A\n","Downloading data:  29% 49.0M/169M [00:04<00:08, 14.4MB/s]\u001b[A\n","Downloading data:  30% 51.6M/169M [00:04<00:08, 14.4MB/s]\u001b[A\n","Downloading data:  32% 54.3M/169M [00:04<00:07, 14.5MB/s]\u001b[A\n","Downloading data:  34% 57.0M/169M [00:05<00:07, 14.5MB/s]\u001b[A\n","Downloading data:  35% 59.7M/169M [00:05<00:07, 14.4MB/s]\u001b[A\n","Downloading data:  37% 62.4M/169M [00:05<00:07, 14.5MB/s]\u001b[A\n","Downloading data:  38% 65.1M/169M [00:05<00:07, 14.5MB/s]\u001b[A\n","Downloading data:  40% 67.8M/169M [00:05<00:06, 14.6MB/s]\u001b[A\n","Downloading data:  42% 70.5M/169M [00:06<00:06, 14.6MB/s]\u001b[A\n","Downloading data:  43% 73.1M/169M [00:06<00:06, 14.5MB/s]\u001b[A\n","Downloading data:  45% 75.9M/169M [00:06<00:06, 14.6MB/s]\u001b[A\n","Downloading data:  46% 78.7M/169M [00:06<00:06, 14.7MB/s]\u001b[A\n","Downloading data:  48% 81.3M/169M [00:06<00:06, 14.6MB/s]\u001b[A\n","Downloading data:  50% 84.0M/169M [00:07<00:05, 14.7MB/s]\u001b[A\n","Downloading data:  51% 86.8M/169M [00:07<00:05, 14.7MB/s]\u001b[A\n","Downloading data:  53% 89.5M/169M [00:07<00:05, 14.8MB/s]\u001b[A\n","Downloading data:  54% 92.1M/169M [00:07<00:05, 14.5MB/s]\u001b[A\n","Downloading data:  56% 94.9M/169M [00:07<00:05, 14.6MB/s]\u001b[A\n","Downloading data:  58% 97.7M/169M [00:07<00:04, 14.9MB/s]\u001b[A\n","Downloading data:  59% 100M/169M [00:08<00:04, 14.5MB/s] \u001b[A\n","Downloading data:  61% 103M/169M [00:08<00:04, 14.7MB/s]\u001b[A\n","Downloading data:  62% 106M/169M [00:08<00:04, 14.7MB/s]\u001b[A\n","Downloading data:  64% 109M/169M [00:08<00:04, 14.8MB/s]\u001b[A\n","Downloading data:  66% 111M/169M [00:08<00:03, 14.7MB/s]\u001b[A\n","Downloading data:  67% 114M/169M [00:09<00:03, 14.8MB/s]\u001b[A\n","Downloading data:  69% 117M/169M [00:09<00:03, 14.8MB/s]\u001b[A\n","Downloading data:  71% 119M/169M [00:09<00:03, 14.8MB/s]\u001b[A\n","Downloading data:  72% 122M/169M [00:09<00:03, 14.9MB/s]\u001b[A\n","Downloading data:  74% 125M/169M [00:09<00:02, 14.9MB/s]\u001b[A\n","Downloading data:  75% 128M/169M [00:09<00:02, 14.9MB/s]\u001b[A\n","Downloading data:  77% 131M/169M [00:10<00:02, 15.0MB/s]\u001b[A\n","Downloading data:  79% 133M/169M [00:10<00:02, 14.9MB/s]\u001b[A\n","Downloading data:  80% 136M/169M [00:10<00:02, 14.8MB/s]\u001b[A\n","Downloading data:  82% 139M/169M [00:10<00:02, 15.0MB/s]\u001b[A\n","Downloading data:  84% 141M/169M [00:10<00:01, 14.8MB/s]\u001b[A\n","Downloading data:  85% 144M/169M [00:11<00:01, 14.9MB/s]\u001b[A\n","Downloading data:  87% 147M/169M [00:11<00:01, 15.0MB/s]\u001b[A\n","Downloading data:  89% 150M/169M [00:11<00:01, 15.1MB/s]\u001b[A\n","Downloading data:  90% 153M/169M [00:11<00:01, 14.9MB/s]\u001b[A\n","Downloading data:  92% 155M/169M [00:11<00:00, 15.0MB/s]\u001b[A\n","Downloading data:  93% 158M/169M [00:11<00:00, 15.2MB/s]\u001b[A\n","Downloading data:  95% 161M/169M [00:12<00:00, 15.2MB/s]\u001b[A\n","Downloading data:  97% 164M/169M [00:12<00:00, 15.3MB/s]\u001b[A\n","Downloading data: 100% 169M/169M [00:12<00:00, 13.5MB/s]\n","Downloading data files: 100% 1/1 [00:14<00:00, 14.34s/it]\n","Extracting data files: 100% 1/1 [00:00<00:00, 1071.07it/s]\n","Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n","Downloading data:   0% 0.00/4.19M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   0% 17.4k/4.19M [00:00<00:44, 93.9kB/s]\u001b[A\n","Downloading data:   1% 52.2k/4.19M [00:00<00:27, 149kB/s] \u001b[A\n","Downloading data:   3% 122k/4.19M [00:00<00:16, 252kB/s] \u001b[A\n","Downloading data:   6% 261k/4.19M [00:00<00:08, 464kB/s]\u001b[A\n","Downloading data:  13% 557k/4.19M [00:00<00:04, 879kB/s]\u001b[A\n","Downloading data:  27% 1.15M/4.19M [00:01<00:01, 1.67MB/s]\u001b[A\n","Downloading data: 100% 4.19M/4.19M [00:01<00:00, 3.17MB/s]\n","Downloading data files: 100% 1/1 [00:03<00:00,  3.03s/it]\n","Extracting data files: 100% 1/1 [00:00<00:00, 1144.73it/s]\n","Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n","Downloading data:   0% 0.00/4.21M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   0% 17.4k/4.21M [00:00<00:44, 94.4kB/s]\u001b[A\n","Downloading data:   1% 52.2k/4.21M [00:00<00:27, 150kB/s] \u001b[A\n","Downloading data:   3% 122k/4.21M [00:00<00:16, 253kB/s] \u001b[A\n","Downloading data:   6% 261k/4.21M [00:00<00:08, 468kB/s]\u001b[A\n","Downloading data:  13% 557k/4.21M [00:00<00:04, 886kB/s]\u001b[A\n","Downloading data:  27% 1.15M/4.21M [00:01<00:01, 1.68MB/s]\u001b[A\n","Downloading data: 100% 4.21M/4.21M [00:01<00:00, 3.21MB/s]\n","Downloading data files: 100% 1/1 [00:02<00:00,  2.96s/it]\n","Extracting data files: 100% 1/1 [00:00<00:00, 1245.71it/s]\n","Dataset amazon_reviews_multi downloaded and prepared to /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609. Subsequent calls will reuse this data.\n","100% 5/5 [00:00<00:00, 92.74ba/s]\n","100% 5/5 [00:00<00:00, 77.56ba/s]\n","100% 2/2 [00:00<00:00,  4.90ba/s]\n","100% 2/2 [00:01<00:00,  1.76ba/s]\n","100% 1/1 [00:00<00:00,  9.92ba/s]\n","100% 1/1 [00:00<00:00,  3.42ba/s]\n","Train data statistics: {'total': 1600, 'positive': 803, 'negative': 797}\n","Test data statistics: {'total': 400, 'positive': 197, 'negative': 203}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body. If reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:44<01:24,  4.75it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body. If reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.89it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.43it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.29it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.81it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.52it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.34it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.21it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.12it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.07it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.96it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.94it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.94it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.93it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.94it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.90it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.3205104172229767, 'eval_accuracy': 0.905, 'eval_precision': 0.8803827751196173, 'eval_recall': 0.934010152284264, 'eval_f1': 0.9064039408866995, 'eval_runtime': 3.1547, 'eval_samples_per_second': 126.797, 'eval_steps_per_second': 15.85, 'epoch': 1.0}\n"," 33% 200/600 [00:47<01:24,  4.75it/s]\n","100% 50/50 [00:03<00:00, 15.91it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:32<00:42,  4.74it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body. If reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.86it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.41it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.41it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.85it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.52it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.30it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.16it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.96it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.91it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.90it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.89it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.89it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.89it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.88it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.88it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.85it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.85it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.81it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.83it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.84it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.3597070276737213, 'eval_accuracy': 0.9225, 'eval_precision': 0.9322916666666666, 'eval_recall': 0.9086294416243654, 'eval_f1': 0.9203084832904885, 'eval_runtime': 3.1563, 'eval_samples_per_second': 126.73, 'eval_steps_per_second': 15.841, 'epoch': 2.0}\n"," 67% 400/600 [01:35<00:42,  4.74it/s]\n","100% 50/50 [00:03<00:00, 15.86it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2602, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:20<00:00,  4.74it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body. If reviewer_id, review_title, product_category, product_id, stars, review_id, language, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.76it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.36it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.36it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.80it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.48it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.24it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.11it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 15.94it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.87it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.87it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.86it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.87it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.87it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.87it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.87it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.86it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.86it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.87it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.87it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.87it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.4863426685333252, 'eval_accuracy': 0.9175, 'eval_precision': 0.9315789473684211, 'eval_recall': 0.8984771573604061, 'eval_f1': 0.9147286821705426, 'eval_runtime': 3.1599, 'eval_samples_per_second': 126.588, 'eval_steps_per_second': 15.823, 'epoch': 3.0}\n","100% 600/600 [02:23<00:00,  4.74it/s]\n","100% 50/50 [00:03<00:00, 15.86it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-200 (score: 0.3205104172229767).\n","{'train_runtime': 146.6474, 'train_samples_per_second': 32.732, 'train_steps_per_second': 4.091, 'train_loss': 0.2362411387761434, 'epoch': 3.0}\n","100% 600/600 [02:26<00:00,  4.09it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2022-12-01 05:21:54.748709: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-83f8d432a1e3b464.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-725ca478270fd589.arrow\n","100% 2/2 [00:00<00:00,  3.57ba/s]\n","100% 2/2 [00:01<00:00,  1.78ba/s]\n","100% 1/1 [00:00<00:00,  9.91ba/s]\n","100% 1/1 [00:00<00:00,  3.45ba/s]\n","Train data statistics: {'total': 1600, 'positive': 804, 'negative': 796}\n","Test data statistics: {'total': 400, 'positive': 196, 'negative': 204}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id. If product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:42<01:24,  4.75it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id. If product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.79it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.40it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.39it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.85it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.50it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.31it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.12it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 15.94it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.94it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.94it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.94it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.87it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.88it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.89it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.89it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.89it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.3629629611968994, 'eval_accuracy': 0.8925, 'eval_precision': 0.8805970149253731, 'eval_recall': 0.9030612244897959, 'eval_f1': 0.8916876574307304, 'eval_runtime': 3.1547, 'eval_samples_per_second': 126.795, 'eval_steps_per_second': 15.849, 'epoch': 1.0}\n"," 33% 200/600 [00:46<01:24,  4.75it/s]\n","100% 50/50 [00:03<00:00, 15.88it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:31<00:42,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id. If product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.87it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.43it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.44it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.90it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.53it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.32it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.19it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.96it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.95it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.93it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.90it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.89it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.89it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.88it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.86it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.86it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.5302600860595703, 'eval_accuracy': 0.8925, 'eval_precision': 0.9047619047619048, 'eval_recall': 0.8724489795918368, 'eval_f1': 0.8883116883116883, 'eval_runtime': 3.152, 'eval_samples_per_second': 126.904, 'eval_steps_per_second': 15.863, 'epoch': 2.0}\n"," 67% 400/600 [01:34<00:42,  4.76it/s]\n","100% 50/50 [00:03<00:00, 15.87it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2623, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:19<00:00,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id. If product_category, review_title, review_body, language, product_id, stars, review_id, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.86it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.43it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.43it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.85it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.53it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.32it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.19it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.96it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.90it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.86it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.87it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.88it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.87it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.86it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.88it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.88it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.82it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.84it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.85it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.84it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.5319191217422485, 'eval_accuracy': 0.8875, 'eval_precision': 0.8756218905472637, 'eval_recall': 0.8979591836734694, 'eval_f1': 0.8866498740554155, 'eval_runtime': 3.1565, 'eval_samples_per_second': 126.722, 'eval_steps_per_second': 15.84, 'epoch': 3.0}\n","100% 600/600 [02:22<00:00,  4.76it/s]\n","100% 50/50 [00:03<00:00, 15.86it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-200 (score: 0.3629629611968994).\n","{'train_runtime': 146.1816, 'train_samples_per_second': 32.836, 'train_steps_per_second': 4.104, 'train_loss': 0.2327947998046875, 'epoch': 3.0}\n","100% 600/600 [02:26<00:00,  4.10it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2022-12-01 05:24:42.010713: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-83f8d432a1e3b464.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-725ca478270fd589.arrow\n","100% 2/2 [00:00<00:00,  3.58ba/s]\n","100% 2/2 [00:01<00:00,  1.77ba/s]\n","100% 1/1 [00:00<00:00,  9.77ba/s]\n","100% 1/1 [00:00<00:00,  3.54ba/s]\n","Train data statistics: {'total': 1600, 'positive': 787, 'negative': 813}\n","Test data statistics: {'total': 400, 'positive': 213, 'negative': 187}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id. If review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:42<01:23,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id. If review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.78it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.39it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.41it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.85it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.52it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.32it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.15it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.06it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.96it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.93it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.91it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.91it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.90it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.90it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.36546266078948975, 'eval_accuracy': 0.8875, 'eval_precision': 0.8589743589743589, 'eval_recall': 0.9436619718309859, 'eval_f1': 0.8993288590604026, 'eval_runtime': 3.1527, 'eval_samples_per_second': 126.876, 'eval_steps_per_second': 15.859, 'epoch': 1.0}\n"," 33% 200/600 [00:46<01:23,  4.76it/s]\n","100% 50/50 [00:03<00:00, 15.90it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:31<00:41,  4.77it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id. If review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.83it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.46it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.47it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.91it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.57it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.37it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.23it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.14it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.07it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.96it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.92it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.90it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.33052876591682434, 'eval_accuracy': 0.9075, 'eval_precision': 0.9271844660194175, 'eval_recall': 0.8967136150234741, 'eval_f1': 0.9116945107398567, 'eval_runtime': 3.1441, 'eval_samples_per_second': 127.222, 'eval_steps_per_second': 15.903, 'epoch': 2.0}\n"," 67% 400/600 [01:34<00:41,  4.77it/s]\n","100% 50/50 [00:03<00:00, 15.91it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2625, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:19<00:00,  4.75it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id. If review_title, product_id, stars, language, reviewer_id, product_category, review_body, review_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.62it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.26it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.32it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.80it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.46it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.27it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.13it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 15.95it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.93it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.90it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.88it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.87it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.85it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.85it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.85it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.86it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.88it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.88it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.90it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.89it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.45344191789627075, 'eval_accuracy': 0.91, 'eval_precision': 0.9359605911330049, 'eval_recall': 0.892018779342723, 'eval_f1': 0.9134615384615385, 'eval_runtime': 3.1592, 'eval_samples_per_second': 126.613, 'eval_steps_per_second': 15.827, 'epoch': 3.0}\n","100% 600/600 [02:22<00:00,  4.75it/s]\n","100% 50/50 [00:03<00:00, 15.89it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-400 (score: 0.33052876591682434).\n","{'train_runtime': 146.1965, 'train_samples_per_second': 32.833, 'train_steps_per_second': 4.104, 'train_loss': 0.23486250559488933, 'epoch': 3.0}\n","100% 600/600 [02:26<00:00,  4.10it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2022-12-01 05:27:29.141655: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-83f8d432a1e3b464.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-725ca478270fd589.arrow\n","100% 2/2 [00:00<00:00,  3.59ba/s]\n","100% 2/2 [00:01<00:00,  1.81ba/s]\n","100% 1/1 [00:00<00:00,  9.78ba/s]\n","100% 1/1 [00:00<00:00,  3.57ba/s]\n","Train data statistics: {'total': 1600, 'positive': 800, 'negative': 800}\n","Test data statistics: {'total': 400, 'positive': 200, 'negative': 200}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id. If review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:42<01:24,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id. If review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.74it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.44it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.46it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.92it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.57it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.35it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.20it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.11it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.06it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.93it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.91it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.93it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.92it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.91it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.91it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.48037025332450867, 'eval_accuracy': 0.89, 'eval_precision': 0.8714285714285714, 'eval_recall': 0.915, 'eval_f1': 0.8926829268292683, 'eval_runtime': 3.1469, 'eval_samples_per_second': 127.107, 'eval_steps_per_second': 15.888, 'epoch': 1.0}\n"," 33% 200/600 [00:46<01:24,  4.76it/s]\n","100% 50/50 [00:03<00:00, 15.91it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:31<00:41,  4.78it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id. If review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.95it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.50it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.52it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.96it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.63it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.43it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.29it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.17it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.05it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.99it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.98it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.39231735467910767, 'eval_accuracy': 0.9, 'eval_precision': 0.8921568627450981, 'eval_recall': 0.91, 'eval_f1': 0.900990099009901, 'eval_runtime': 3.1345, 'eval_samples_per_second': 127.612, 'eval_steps_per_second': 15.952, 'epoch': 2.0}\n"," 67% 400/600 [01:34<00:41,  4.78it/s]\n","100% 50/50 [00:03<00:00, 15.99it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2588, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:19<00:00,  4.77it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id. If review_id, review_title, product_id, stars, language, product_category, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.95it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.50it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.50it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.95it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.62it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.41it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.27it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.19it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.12it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.08it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.05it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.99it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.96it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.4439573585987091, 'eval_accuracy': 0.9125, 'eval_precision': 0.914572864321608, 'eval_recall': 0.91, 'eval_f1': 0.912280701754386, 'eval_runtime': 3.135, 'eval_samples_per_second': 127.593, 'eval_steps_per_second': 15.949, 'epoch': 3.0}\n","100% 600/600 [02:22<00:00,  4.77it/s]\n","100% 50/50 [00:03<00:00, 15.95it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-400 (score: 0.39231735467910767).\n","{'train_runtime': 145.9125, 'train_samples_per_second': 32.896, 'train_steps_per_second': 4.112, 'train_loss': 0.2274807325998942, 'epoch': 3.0}\n","100% 600/600 [02:25<00:00,  4.11it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2022-12-01 05:30:16.837176: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-83f8d432a1e3b464.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-725ca478270fd589.arrow\n","100% 2/2 [00:00<00:00,  3.64ba/s]\n","100% 2/2 [00:01<00:00,  1.78ba/s]\n","100% 1/1 [00:00<00:00,  9.95ba/s]\n","100% 1/1 [00:00<00:00,  3.49ba/s]\n","Train data statistics: {'total': 1600, 'positive': 797, 'negative': 803}\n","Test data statistics: {'total': 400, 'positive': 203, 'negative': 197}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title. If review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:42<01:24,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title. If review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.91it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.50it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.47it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.93it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.60it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.39it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.26it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.16it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.11it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.06it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.05it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 16.00it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.97it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.24824093282222748, 'eval_accuracy': 0.9125, 'eval_precision': 0.8925233644859814, 'eval_recall': 0.9408866995073891, 'eval_f1': 0.9160671462829737, 'eval_runtime': 3.1364, 'eval_samples_per_second': 127.537, 'eval_steps_per_second': 15.942, 'epoch': 1.0}\n"," 33% 200/600 [00:45<01:24,  4.76it/s]\n","100% 50/50 [00:03<00:00, 15.96it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:30<00:41,  4.77it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title. If review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.95it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.50it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.50it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.95it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.61it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.40it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.25it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.16it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.05it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.94it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.95it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.3485160768032074, 'eval_accuracy': 0.905, 'eval_precision': 0.9319371727748691, 'eval_recall': 0.8768472906403941, 'eval_f1': 0.9035532994923858, 'eval_runtime': 3.1394, 'eval_samples_per_second': 127.413, 'eval_steps_per_second': 15.927, 'epoch': 2.0}\n"," 67% 400/600 [01:34<00:41,  4.77it/s]\n","100% 50/50 [00:03<00:00, 15.95it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2987, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:19<00:00,  4.77it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title. If review_id, stars, review_body, language, reviewer_id, product_category, product_id, review_title are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.80it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.46it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.50it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.97it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.63it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.41it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.27it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.17it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.11it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.06it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.97it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.97it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.43358275294303894, 'eval_accuracy': 0.9025, 'eval_precision': 0.8942307692307693, 'eval_recall': 0.916256157635468, 'eval_f1': 0.9051094890510949, 'eval_runtime': 3.1372, 'eval_samples_per_second': 127.501, 'eval_steps_per_second': 15.938, 'epoch': 3.0}\n","100% 600/600 [02:22<00:00,  4.77it/s]\n","100% 50/50 [00:03<00:00, 15.97it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-200 (score: 0.24824093282222748).\n","{'train_runtime': 145.6462, 'train_samples_per_second': 32.957, 'train_steps_per_second': 4.12, 'train_loss': 0.2666356674830119, 'epoch': 3.0}\n","100% 600/600 [02:25<00:00,  4.12it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2022-12-01 05:33:03.094677: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-83f8d432a1e3b464.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-725ca478270fd589.arrow\n","100% 2/2 [00:00<00:00,  3.54ba/s]\n","100% 2/2 [00:01<00:00,  1.78ba/s]\n","100% 1/1 [00:00<00:00,  9.91ba/s]\n","100% 1/1 [00:00<00:00,  3.50ba/s]\n","Train data statistics: {'total': 1600, 'positive': 797, 'negative': 803}\n","Test data statistics: {'total': 400, 'positive': 203, 'negative': 197}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id. If review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:42<01:23,  4.78it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id. If review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.97it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.53it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.54it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.99it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.62it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.41it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.27it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.19it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.13it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.08it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.06it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 68% 34/50 [00:02<00:00, 16.02it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 16.01it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 16.00it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 16.00it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 16.00it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 16.01it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.38308823108673096, 'eval_accuracy': 0.87, 'eval_precision': 0.8952879581151832, 'eval_recall': 0.8423645320197044, 'eval_f1': 0.868020304568528, 'eval_runtime': 3.1309, 'eval_samples_per_second': 127.757, 'eval_steps_per_second': 15.97, 'epoch': 1.0}\n"," 33% 200/600 [00:45<01:23,  4.78it/s]\n","100% 50/50 [00:03<00:00, 16.01it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:30<00:41,  4.78it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id. If review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.97it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.54it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.54it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.98it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.64it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.43it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.28it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.18it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.11it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.08it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.97it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.97it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.5432677268981934, 'eval_accuracy': 0.88, 'eval_precision': 0.8414096916299559, 'eval_recall': 0.9408866995073891, 'eval_f1': 0.8883720930232558, 'eval_runtime': 3.1359, 'eval_samples_per_second': 127.557, 'eval_steps_per_second': 15.945, 'epoch': 2.0}\n"," 67% 400/600 [01:33<00:41,  4.78it/s]\n","100% 50/50 [00:03<00:00, 15.95it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2707, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:18<00:00,  4.78it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id. If review_body, product_id, review_id, stars, product_category, review_title, language, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.97it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.54it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.54it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.99it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.65it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.43it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.30it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.20it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.13it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.06it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.99it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.97it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.5215829610824585, 'eval_accuracy': 0.8975, 'eval_precision': 0.900990099009901, 'eval_recall': 0.896551724137931, 'eval_f1': 0.8987654320987656, 'eval_runtime': 3.1319, 'eval_samples_per_second': 127.717, 'eval_steps_per_second': 15.965, 'epoch': 3.0}\n","100% 600/600 [02:21<00:00,  4.78it/s]\n","100% 50/50 [00:03<00:00, 15.98it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-200 (score: 0.38308823108673096).\n","{'train_runtime': 146.0972, 'train_samples_per_second': 32.855, 'train_steps_per_second': 4.107, 'train_loss': 0.2395570453008016, 'epoch': 3.0}\n","100% 600/600 [02:26<00:00,  4.11it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2022-12-01 05:35:51.282513: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-83f8d432a1e3b464.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-725ca478270fd589.arrow\n","100% 2/2 [00:00<00:00,  3.64ba/s]\n","100% 2/2 [00:01<00:00,  1.78ba/s]\n","100% 1/1 [00:00<00:00,  9.94ba/s]\n","100% 1/1 [00:00<00:00,  3.50ba/s]\n","Train data statistics: {'total': 1600, 'positive': 809, 'negative': 791}\n","Test data statistics: {'total': 400, 'positive': 191, 'negative': 209}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category. If product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:42<01:23,  4.77it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category. If product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.93it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.52it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.53it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.95it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.62it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.41it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.28it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.17it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.11it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.08it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.06it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.99it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 16.00it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 16.00it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.99it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.98it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.2778049409389496, 'eval_accuracy': 0.9075, 'eval_precision': 0.9010416666666666, 'eval_recall': 0.9057591623036649, 'eval_f1': 0.9033942558746737, 'eval_runtime': 3.1343, 'eval_samples_per_second': 127.62, 'eval_steps_per_second': 15.953, 'epoch': 1.0}\n"," 33% 200/600 [00:45<01:23,  4.77it/s]\n","100% 50/50 [00:03<00:00, 15.99it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:30<00:41,  4.78it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category. If product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.88it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.47it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.48it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.94it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.61it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.39it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.25it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.16it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.10it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.07it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.03it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.98it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.96it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.41465967893600464, 'eval_accuracy': 0.9025, 'eval_precision': 0.8653846153846154, 'eval_recall': 0.9424083769633508, 'eval_f1': 0.9022556390977444, 'eval_runtime': 3.1366, 'eval_samples_per_second': 127.525, 'eval_steps_per_second': 15.941, 'epoch': 2.0}\n"," 67% 400/600 [01:34<00:41,  4.78it/s]\n","100% 50/50 [00:03<00:00, 15.97it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2926, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:19<00:00,  4.78it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category. If product_id, reviewer_id, stars, language, review_body, review_title, review_id, product_category are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 24.00it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.50it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.49it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.94it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.61it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.40it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.27it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.16it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.10it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.07it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.05it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.98it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.98it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.97it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.4798755645751953, 'eval_accuracy': 0.9075, 'eval_precision': 0.8774509803921569, 'eval_recall': 0.93717277486911, 'eval_f1': 0.9063291139240506, 'eval_runtime': 3.1351, 'eval_samples_per_second': 127.587, 'eval_steps_per_second': 15.948, 'epoch': 3.0}\n","100% 600/600 [02:22<00:00,  4.78it/s]\n","100% 50/50 [00:03<00:00, 15.98it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-200 (score: 0.2778049409389496).\n","{'train_runtime': 145.9576, 'train_samples_per_second': 32.886, 'train_steps_per_second': 4.111, 'train_loss': 0.2607997226715088, 'epoch': 3.0}\n","100% 600/600 [02:25<00:00,  4.11it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2022-12-01 05:38:37.882937: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:datasets.builder:Found cached dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-83f8d432a1e3b464.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/ja/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609/cache-725ca478270fd589.arrow\n","100% 2/2 [00:00<00:00,  3.62ba/s]\n","100% 2/2 [00:01<00:00,  1.81ba/s]\n","100% 1/1 [00:00<00:00,  9.97ba/s]\n","100% 1/1 [00:00<00:00,  3.58ba/s]\n","Train data statistics: {'total': 1600, 'positive': 806, 'negative': 794}\n","Test data statistics: {'total': 400, 'positive': 194, 'negative': 206}\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body. If language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 600\n","  Number of trainable parameters = 110618882\n"," 33% 200/600 [00:42<01:23,  4.77it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body. If language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.88it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.44it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.46it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.92it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.60it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.39it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.25it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.15it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.02it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.97it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.97it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.97it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.96it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.93it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.31772053241729736, 'eval_accuracy': 0.8825, 'eval_precision': 0.8483412322274881, 'eval_recall': 0.9226804123711341, 'eval_f1': 0.8839506172839506, 'eval_runtime': 3.1408, 'eval_samples_per_second': 127.357, 'eval_steps_per_second': 15.92, 'epoch': 1.0}\n"," 33% 200/600 [00:45<01:23,  4.77it/s]\n","100% 50/50 [00:03<00:00, 15.94it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-200\n","Configuration saved in output/checkpoint-200/config.json\n","Model weights saved in output/checkpoint-200/pytorch_model.bin\n"," 67% 400/600 [01:33<00:41,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body. If language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.87it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.48it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.49it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.93it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.60it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.39it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.26it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.16it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.09it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.05it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 16.00it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.98it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.96it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.94it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.95it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.93it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.502943217754364, 'eval_accuracy': 0.88, 'eval_precision': 0.8762886597938144, 'eval_recall': 0.8762886597938144, 'eval_f1': 0.8762886597938144, 'eval_runtime': 3.1404, 'eval_samples_per_second': 127.372, 'eval_steps_per_second': 15.922, 'epoch': 2.0}\n"," 67% 400/600 [01:37<00:41,  4.76it/s]\n","100% 50/50 [00:03<00:00, 15.94it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-400\n","Configuration saved in output/checkpoint-400/config.json\n","Model weights saved in output/checkpoint-400/pytorch_model.bin\n","{'loss': 0.2542, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n","100% 600/600 [02:22<00:00,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body. If language, product_id, product_category, reviewer_id, stars, review_title, review_id, review_body are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 400\n","  Batch size = 8\n","\n","  0% 0/50 [00:00<?, ?it/s]\u001b[A\n","  6% 3/50 [00:00<00:01, 23.93it/s]\u001b[A\n"," 12% 6/50 [00:00<00:02, 18.50it/s]\u001b[A\n"," 16% 8/50 [00:00<00:02, 17.49it/s]\u001b[A\n"," 20% 10/50 [00:00<00:02, 16.90it/s]\u001b[A\n"," 24% 12/50 [00:00<00:02, 16.58it/s]\u001b[A\n"," 28% 14/50 [00:00<00:02, 16.36it/s]\u001b[A\n"," 32% 16/50 [00:00<00:02, 16.22it/s]\u001b[A\n"," 36% 18/50 [00:01<00:01, 16.14it/s]\u001b[A\n"," 40% 20/50 [00:01<00:01, 16.07it/s]\u001b[A\n"," 44% 22/50 [00:01<00:01, 16.04it/s]\u001b[A\n"," 48% 24/50 [00:01<00:01, 16.01it/s]\u001b[A\n"," 52% 26/50 [00:01<00:01, 15.99it/s]\u001b[A\n"," 56% 28/50 [00:01<00:01, 15.95it/s]\u001b[A\n"," 60% 30/50 [00:01<00:01, 15.93it/s]\u001b[A\n"," 64% 32/50 [00:01<00:01, 15.92it/s]\u001b[A\n"," 68% 34/50 [00:02<00:01, 15.92it/s]\u001b[A\n"," 72% 36/50 [00:02<00:00, 15.93it/s]\u001b[A\n"," 76% 38/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 80% 40/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 84% 42/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 88% 44/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 92% 46/50 [00:02<00:00, 15.94it/s]\u001b[A\n"," 96% 48/50 [00:02<00:00, 15.93it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.6033396124839783, 'eval_accuracy': 0.8875, 'eval_precision': 0.8743718592964824, 'eval_recall': 0.8969072164948454, 'eval_f1': 0.8854961832061069, 'eval_runtime': 3.1431, 'eval_samples_per_second': 127.261, 'eval_steps_per_second': 15.908, 'epoch': 3.0}\n","100% 600/600 [02:25<00:00,  4.76it/s]\n","100% 50/50 [00:03<00:00, 15.93it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to output/checkpoint-600\n","Configuration saved in output/checkpoint-600/config.json\n","Model weights saved in output/checkpoint-600/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from output/checkpoint-200 (score: 0.31772053241729736).\n","{'train_runtime': 148.6892, 'train_samples_per_second': 32.282, 'train_steps_per_second': 4.035, 'train_loss': 0.221584951877594, 'epoch': 3.0}\n","100% 600/600 [02:28<00:00,  4.04it/s]\n","/usr/bin/python3: Error while finding module specification for 'scripts.nlp.finetune.py' (ModuleNotFoundError: __path__ attribute not found on 'scripts.nlp.finetune' while trying to find 'scripts.nlp.finetune.py')\n","2min 47s ± 892 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}],"source":["%%timeit\n","!python -m scripts.nlp.finetune.py"]},{"cell_type":"code","source":[],"metadata":{"id":"b_I-XpC8qzmo","executionInfo":{"status":"ok","timestamp":1669873287622,"user_tz":-540,"elapsed":19,"user":{"displayName":"Takahiro Kubo","userId":"12892517346285462954"}}},"execution_count":6,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/icoxfog417/notebook-bench/blob/main/notebooks/nlp.ipynb","timestamp":1669871672706}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}