{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook benchmark for NLP Task\n\n* Dataset: [amazon_reviews_multi](https://huggingface.co/datasets/amazon_reviews_multi)\n   * Language: ja\n* Task: Binary classification\n   * We use star 1 and 5\n\n## Set up\n\n### For Kaggle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-08T01:03:00.437459Z","iopub.execute_input":"2022-12-08T01:03:00.437843Z","iopub.status.idle":"2022-12-08T01:03:01.943837Z","shell.execute_reply.started":"2022-12-08T01:03:00.437804Z","shell.execute_reply":"2022-12-08T01:03:01.942518Z"}}},{"cell_type":"code","source":"!git clone https://github.com/icoxfog417/notebook-bench.git","metadata":{"execution":{"iopub.status.busy":"2022-12-08T01:16:01.923601Z","iopub.execute_input":"2022-12-08T01:16:01.924229Z","iopub.status.idle":"2022-12-08T01:16:02.935269Z","shell.execute_reply.started":"2022-12-08T01:16:01.924160Z","shell.execute_reply":"2022-12-08T01:16:02.934013Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'notebook-bench' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install transformers[torch]==4.24.0 fugashi==1.2.0 ipadic==1.0.0 datasets==2.7.1 sentencepiece==0.1.97","metadata":{"execution":{"iopub.status.busy":"2022-12-08T01:16:02.942312Z","iopub.execute_input":"2022-12-08T01:16:02.942692Z","iopub.status.idle":"2022-12-08T01:16:13.677316Z","shell.execute_reply.started":"2022-12-08T01:16:02.942647Z","shell.execute_reply":"2022-12-08T01:16:13.675919Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch]==4.24.0 in /opt/conda/lib/python3.7/site-packages (4.24.0)\nRequirement already satisfied: fugashi==1.2.0 in /opt/conda/lib/python3.7/site-packages (1.2.0)\nRequirement already satisfied: ipadic==1.0.0 in /opt/conda/lib/python3.7/site-packages (1.0.0)\nRequirement already satisfied: datasets==2.7.1 in /opt/conda/lib/python3.7/site-packages (2.7.1)\nRequirement already satisfied: sentencepiece==0.1.97 in /opt/conda/lib/python3.7/site-packages (0.1.97)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (4.13.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (4.64.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (3.7.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (0.12.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (0.10.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (6.0)\nRequirement already satisfied: torch!=1.12.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from transformers[torch]==4.24.0) (1.11.0)\nRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (0.3.5.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (3.8.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (0.70.13)\nRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (10.0.1)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (2022.8.2)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (0.18.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (3.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==2.7.1) (1.3.5)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (4.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (21.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (0.13.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (2.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (1.7.2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (4.1.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.7.1) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers[torch]==4.24.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[torch]==4.24.0) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[torch]==4.24.0) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers[torch]==4.24.0) (2022.9.24)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers[torch]==4.24.0) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.7.1) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.7.1) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.7.1) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Calculation","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-12-08T01:16:13.679591Z","iopub.execute_input":"2022-12-08T01:16:13.680049Z","iopub.status.idle":"2022-12-08T01:16:14.826057Z","shell.execute_reply.started":"2022-12-08T01:16:13.680004Z","shell.execute_reply":"2022-12-08T01:16:14.824787Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Thu Dec  8 01:16:14 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-12-08T01:16:14.830002Z","iopub.execute_input":"2022-12-08T01:16:14.830394Z","iopub.status.idle":"2022-12-08T01:16:15.844291Z","shell.execute_reply.started":"2022-12-08T01:16:14.830360Z","shell.execute_reply":"2022-12-08T01:16:15.843155Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb  notebook-bench\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport os\nfrom pathlib import Path\n\n\nif Path.cwd().name != \"notebook-bench\":\n    os.chdir(Path.cwd().joinpath(\"notebook-bench\"))\n\nprint(Path.cwd())","metadata":{"execution":{"iopub.status.busy":"2022-12-08T01:16:15.847795Z","iopub.execute_input":"2022-12-08T01:16:15.848195Z","iopub.status.idle":"2022-12-08T01:16:15.856022Z","shell.execute_reply.started":"2022-12-08T01:16:15.848162Z","shell.execute_reply":"2022-12-08T01:16:15.855038Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working/notebook-bench\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-12-08T01:16:15.857572Z","iopub.execute_input":"2022-12-08T01:16:15.858320Z","iopub.status.idle":"2022-12-08T01:16:15.864508Z","shell.execute_reply.started":"2022-12-08T01:16:15.858280Z","shell.execute_reply":"2022-12-08T01:16:15.863399Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%timeit\n!python -m scripts.nlp.finetune","metadata":{"execution":{"iopub.status.busy":"2022-12-08T01:16:15.866079Z","iopub.execute_input":"2022-12-08T01:16:15.866775Z","iopub.status.idle":"2022-12-08T01:59:03.780393Z","shell.execute_reply.started":"2022-12-08T01:16:15.866738Z","shell.execute_reply":"2022-12-08T01:59:03.779031Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  4.07ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.34ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.09ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.23ba/s]\nTrain data statistics: {'total': 1600, 'positive': 809, 'negative': 791}\nTest data statistics: {'total': 400, 'positive': 191, 'negative': 209}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language. If stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:24<02:47,  1.19it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language. If stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.32it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.30it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:00<00:05,  3.77it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.52it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.35it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:01<00:05,  3.26it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.20it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.15it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:02<00:04,  3.12it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.11it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.10it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:03<00:03,  3.08it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  3.09it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  3.08it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:04<00:02,  3.08it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  3.08it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  3.07it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:05<00:01,  3.08it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  3.09it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  3.07it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:06<00:00,  3.07it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  3.06it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  3.11it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.23085545003414154, 'eval_accuracy': 0.9175, 'eval_precision': 0.898989898989899, 'eval_recall': 0.9319371727748691, 'eval_f1': 0.9151670951156812, 'eval_runtime': 8.1152, 'eval_samples_per_second': 49.29, 'eval_steps_per_second': 3.081, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:32<02:47,  1.19it/s]\n100%|███████████████████████████████████████████| 25/25 [00:07<00:00,  3.09it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:02<01:26,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language. If stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.09it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.10it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.63it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.41it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.24it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.17it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.11it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.07it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.05it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.04it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.03it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:03,  3.01it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  3.01it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.99it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.98it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.99it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.98it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.99it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.98it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.99it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.97it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.97it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.98it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2697984576225281, 'eval_accuracy': 0.9175, 'eval_precision': 0.9072164948453608, 'eval_recall': 0.9214659685863874, 'eval_f1': 0.9142857142857143, 'eval_runtime': 8.3829, 'eval_samples_per_second': 47.716, 'eval_steps_per_second': 2.982, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:11<01:26,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:42<00:00,  1.16it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language. If stars, review_title, review_id, review_body, product_id, product_category, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.15it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.06it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.60it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.39it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.14it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.09it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.04it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.03it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.00it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.99it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.99it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.96it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.95it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.96it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.95it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.94it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.94it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.92it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.90it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.89it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.90it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.91it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.39498671889305115, 'eval_accuracy': 0.915, 'eval_precision': 0.9243243243243243, 'eval_recall': 0.8952879581151832, 'eval_f1': 0.9095744680851063, 'eval_runtime': 8.4898, 'eval_samples_per_second': 47.116, 'eval_steps_per_second': 2.945, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:50<00:00,  1.16it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.93it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-100 (score: 0.23085545003414154).\n{'train_runtime': 296.0239, 'train_samples_per_second': 16.215, 'train_steps_per_second': 1.013, 'train_loss': 0.20493175506591796, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:56<00:00,  1.01it/s]\nSome weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.89ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.61ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.98ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.21ba/s]\nTrain data statistics: {'total': 1600, 'positive': 796, 'negative': 804}\nTest data statistics: {'total': 400, 'positive': 204, 'negative': 196}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language. If product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:30<02:50,  1.17it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language. If product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.08it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.08it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.62it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.41it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.15it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.11it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.07it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.04it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.01it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.00it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.97it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.96it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.96it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.96it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.96it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.95it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.98it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.96it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.97it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.97it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.95it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.37275832891464233, 'eval_accuracy': 0.8625, 'eval_precision': 0.8669950738916257, 'eval_recall': 0.8627450980392157, 'eval_f1': 0.8648648648648649, 'eval_runtime': 8.4233, 'eval_samples_per_second': 47.487, 'eval_steps_per_second': 2.968, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:38<02:50,  1.17it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:09<01:27,  1.14it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language. If product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:04,  5.57it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  3.84it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:06,  3.42it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:06,  3.23it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:06,  3.12it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.05it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.02it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.00it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.97it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.96it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.95it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.95it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.95it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.94it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.94it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.94it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.94it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.93it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.93it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.93it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.93it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.94it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.46096694469451904, 'eval_accuracy': 0.8775, 'eval_precision': 0.8538812785388128, 'eval_recall': 0.9166666666666666, 'eval_f1': 0.8841607565011821, 'eval_runtime': 8.5855, 'eval_samples_per_second': 46.59, 'eval_steps_per_second': 2.912, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:18<01:27,  1.14it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.93it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:48<00:00,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language. If product_id, stars, product_category, review_id, review_title, review_body, reviewer_id, language are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  5.96it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.04it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.58it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.38it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.21it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.14it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.08it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.05it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.01it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.99it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.00it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.98it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.97it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.99it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.98it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.98it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.97it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.97it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.98it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.96it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.97it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.97it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.95it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5909499526023865, 'eval_accuracy': 0.875, 'eval_precision': 0.8701923076923077, 'eval_recall': 0.8872549019607843, 'eval_f1': 0.8786407766990291, 'eval_runtime': 8.438, 'eval_samples_per_second': 47.405, 'eval_steps_per_second': 2.963, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:57<00:00,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-100 (score: 0.37275832891464233).\n{'train_runtime': 301.1815, 'train_samples_per_second': 15.937, 'train_steps_per_second': 0.996, 'train_loss': 0.21982958475748698, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [05:01<00:00,  1.00s/it]\nSome weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.91ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.62ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.07ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.30ba/s]\nTrain data statistics: {'total': 1600, 'positive': 792, 'negative': 808}\nTest data statistics: {'total': 400, 'positive': 208, 'negative': 192}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id. If product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:30<02:51,  1.17it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id. If product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.25it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.15it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.64it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.42it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.24it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.15it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.12it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.07it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.05it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.03it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.03it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:03,  3.01it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  3.01it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.97it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.97it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.96it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.96it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.98it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.97it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.96it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.98it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.97it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.41363224387168884, 'eval_accuracy': 0.8225, 'eval_precision': 0.9790209790209791, 'eval_recall': 0.6730769230769231, 'eval_f1': 0.7977207977207977, 'eval_runtime': 8.3842, 'eval_samples_per_second': 47.709, 'eval_steps_per_second': 2.982, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:38<02:51,  1.17it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:09<01:26,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id. If product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.11it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.03it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.58it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.36it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.18it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.09it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.05it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.03it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.99it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.98it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.98it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.97it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.95it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.94it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.92it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.93it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.94it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.93it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.93it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.94it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.95it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.94it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.93it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.30890005826950073, 'eval_accuracy': 0.9, 'eval_precision': 0.9565217391304348, 'eval_recall': 0.8461538461538461, 'eval_f1': 0.8979591836734695, 'eval_runtime': 8.5003, 'eval_samples_per_second': 47.057, 'eval_steps_per_second': 2.941, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:17<01:26,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.95it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:47<00:00,  1.16it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id. If product_category, language, review_id, stars, review_body, product_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.14it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.07it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.60it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.40it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.21it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.13it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.09it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.04it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.02it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.00it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.99it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.99it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.97it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.95it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.95it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.95it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.94it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.95it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.97it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.95it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.94it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.94it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.95it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.35259637236595154, 'eval_accuracy': 0.905, 'eval_precision': 0.9086538461538461, 'eval_recall': 0.9086538461538461, 'eval_f1': 0.9086538461538461, 'eval_runtime': 8.4585, 'eval_samples_per_second': 47.29, 'eval_steps_per_second': 2.956, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:56<00:00,  1.16it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-200 (score: 0.30890005826950073).\n{'train_runtime': 300.3359, 'train_samples_per_second': 15.982, 'train_steps_per_second': 0.999, 'train_loss': 0.22132832845052083, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [05:00<00:00,  1.00s/it]\nSome weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  4.09ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.61ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.26ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.31ba/s]\nTrain data statistics: {'total': 1600, 'positive': 800, 'negative': 800}\nTest data statistics: {'total': 400, 'positive': 200, 'negative': 200}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id. If review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:30<02:51,  1.17it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id. If review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.18it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.13it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.64it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.43it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.25it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.17it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.12it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.08it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.05it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.04it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.03it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:03,  3.00it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.98it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.98it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.96it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.98it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.97it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.95it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.97it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.97it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.97it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.98it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.98it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.25445756316185, 'eval_accuracy': 0.92, 'eval_precision': 0.9038461538461539, 'eval_recall': 0.94, 'eval_f1': 0.9215686274509804, 'eval_runtime': 8.3796, 'eval_samples_per_second': 47.735, 'eval_steps_per_second': 2.983, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:38<02:51,  1.17it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.98it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:09<01:26,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id. If review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.13it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.05it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.58it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.38it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.19it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.10it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.06it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.03it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  3.00it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.98it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.98it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.96it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.95it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.93it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.94it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.96it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.95it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.93it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.94it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.95it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.95it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.94it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.32591167092323303, 'eval_accuracy': 0.9075, 'eval_precision': 0.8790697674418605, 'eval_recall': 0.945, 'eval_f1': 0.9108433734939759, 'eval_runtime': 8.4876, 'eval_samples_per_second': 47.127, 'eval_steps_per_second': 2.945, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:17<01:26,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.95it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:48<00:00,  1.16it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id. If review_body, stars, review_title, language, review_id, product_category, reviewer_id, product_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.11it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.08it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.61it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.39it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.14it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.09it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.05it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.01it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.00it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.98it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.97it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.95it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.96it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.98it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.97it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.98it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.98it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.98it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.97it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.98it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.97it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.41803744435310364, 'eval_accuracy': 0.9175, 'eval_precision': 0.8883720930232558, 'eval_recall': 0.955, 'eval_f1': 0.9204819277108434, 'eval_runtime': 8.423, 'eval_samples_per_second': 47.489, 'eval_steps_per_second': 2.968, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:57<00:00,  1.16it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-100 (score: 0.25445756316185).\n{'train_runtime': 301.1377, 'train_samples_per_second': 15.94, 'train_steps_per_second': 0.996, 'train_loss': 0.18621041615804038, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [05:01<00:00,  1.00s/it]\nSome weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  4.08ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.34ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.07ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.35ba/s]\nTrain data statistics: {'total': 1600, 'positive': 799, 'negative': 801}\nTest data statistics: {'total': 400, 'positive': 201, 'negative': 199}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id. If product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:30<02:50,  1.17it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id. If product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.23it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.12it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.63it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.40it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.15it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.11it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.05it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.01it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.01it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.00it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  3.00it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.98it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.98it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.97it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.96it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.98it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.96it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.95it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.97it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.96it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.96it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.279353529214859, 'eval_accuracy': 0.885, 'eval_precision': 0.9057591623036649, 'eval_recall': 0.8606965174129353, 'eval_f1': 0.8826530612244897, 'eval_runtime': 8.4126, 'eval_samples_per_second': 47.548, 'eval_steps_per_second': 2.972, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:38<02:50,  1.17it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:08<01:26,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id. If product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.15it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.08it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.61it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.39it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.20it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.11it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.06it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.00it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.97it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.96it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.96it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.95it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.94it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.94it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.95it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.94it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.93it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.93it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.93it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.93it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.92it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.94it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.93it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3031890392303467, 'eval_accuracy': 0.905, 'eval_precision': 0.9014778325123153, 'eval_recall': 0.9104477611940298, 'eval_f1': 0.905940594059406, 'eval_runtime': 8.5158, 'eval_samples_per_second': 46.971, 'eval_steps_per_second': 2.936, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:17<01:26,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.92it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:48<00:00,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id. If product_id, product_category, stars, review_body, language, review_id, review_title, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.07it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.05it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.59it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.38it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.18it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.10it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.06it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  2.99it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.98it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.97it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.96it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.96it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.96it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.95it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.94it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.94it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.95it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.95it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.94it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.94it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.95it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.94it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.93it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4312365651130676, 'eval_accuracy': 0.9, 'eval_precision': 0.9086294416243654, 'eval_recall': 0.8905472636815921, 'eval_f1': 0.8994974874371859, 'eval_runtime': 8.4992, 'eval_samples_per_second': 47.063, 'eval_steps_per_second': 2.941, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:56<00:00,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-100 (score: 0.279353529214859).\n{'train_runtime': 300.7235, 'train_samples_per_second': 15.962, 'train_steps_per_second': 0.998, 'train_loss': 0.22003687540690103, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [05:00<00:00,  1.00s/it]\nSome weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.74ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.66ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.32ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.31ba/s]\nTrain data statistics: {'total': 1600, 'positive': 810, 'negative': 790}\nTest data statistics: {'total': 400, 'positive': 190, 'negative': 210}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars. If reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:29<02:51,  1.17it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars. If reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.19it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.09it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.63it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.42it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.21it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.11it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.09it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.04it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.03it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.02it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.01it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  3.00it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  3.00it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.98it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.98it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.98it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.97it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.98it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.96it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.97it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.97it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.96it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.20655551552772522, 'eval_accuracy': 0.925, 'eval_precision': 0.9081632653061225, 'eval_recall': 0.9368421052631579, 'eval_f1': 0.922279792746114, 'eval_runtime': 8.4049, 'eval_samples_per_second': 47.591, 'eval_steps_per_second': 2.974, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:38<02:51,  1.17it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:08<01:26,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars. If reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.13it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.07it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.59it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.38it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:06,  3.14it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.06it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.02it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.00it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.99it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.97it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.96it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.98it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.95it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.95it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.95it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.95it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.94it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.94it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.94it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.95it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.94it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.93it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.25686416029930115, 'eval_accuracy': 0.905, 'eval_precision': 0.9222222222222223, 'eval_recall': 0.8736842105263158, 'eval_f1': 0.8972972972972973, 'eval_runtime': 8.5053, 'eval_samples_per_second': 47.03, 'eval_steps_per_second': 2.939, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:17<01:26,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:47<00:00,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars. If reviewer_id, language, review_id, review_body, product_id, review_title, product_category, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.07it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.05it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.60it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.36it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.18it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.11it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.05it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.01it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  3.00it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.99it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.97it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.96it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.97it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.96it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.94it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.96it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.96it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.94it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.95it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.96it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.95it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.95it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3713512420654297, 'eval_accuracy': 0.9125, 'eval_precision': 0.918918918918919, 'eval_recall': 0.8947368421052632, 'eval_f1': 0.9066666666666667, 'eval_runtime': 8.4778, 'eval_samples_per_second': 47.182, 'eval_steps_per_second': 2.949, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:56<00:00,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.95it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-100 (score: 0.20655551552772522).\n{'train_runtime': 301.0581, 'train_samples_per_second': 15.944, 'train_steps_per_second': 0.996, 'train_loss': 0.21969868977864584, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [05:01<00:00,  1.00s/it]\nSome weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  4.15ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.71ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.24ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.28ba/s]\nTrain data statistics: {'total': 1600, 'positive': 797, 'negative': 803}\nTest data statistics: {'total': 400, 'positive': 203, 'negative': 197}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id. If review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:30<02:50,  1.17it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id. If review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.25it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.06it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.62it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.40it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.14it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.10it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.06it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.04it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.02it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.01it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.99it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.99it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.98it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.98it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.97it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.97it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.97it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.97it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.97it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.98it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.97it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.98it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3712840676307678, 'eval_accuracy': 0.8825, 'eval_precision': 0.90625, 'eval_recall': 0.8571428571428571, 'eval_f1': 0.8810126582278479, 'eval_runtime': 8.3978, 'eval_samples_per_second': 47.632, 'eval_steps_per_second': 2.977, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:38<02:50,  1.17it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:09<01:26,  1.16it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id. If review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.12it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  3.93it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.53it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:06,  3.32it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.17it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.11it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.06it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.02it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  3.00it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.99it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.98it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.97it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.96it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.97it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.96it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.95it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.94it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.96it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.95it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.93it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.94it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.95it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.49831706285476685, 'eval_accuracy': 0.885, 'eval_precision': 0.9025641025641026, 'eval_recall': 0.8669950738916257, 'eval_f1': 0.8844221105527638, 'eval_runtime': 8.4966, 'eval_samples_per_second': 47.078, 'eval_steps_per_second': 2.942, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:17<01:26,  1.16it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:48<00:00,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id. If review_id, product_id, stars, language, product_category, review_title, review_body, reviewer_id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.02it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.01it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.56it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.37it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.19it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.10it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.07it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.03it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  3.00it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.00it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.99it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.97it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.96it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.98it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.96it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.93it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.93it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.90it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.91it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.91it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.92it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.93it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.95it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5339886546134949, 'eval_accuracy': 0.89, 'eval_precision': 0.9076923076923077, 'eval_recall': 0.8719211822660099, 'eval_f1': 0.8894472361809046, 'eval_runtime': 8.5167, 'eval_samples_per_second': 46.967, 'eval_steps_per_second': 2.935, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:57<00:00,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.94it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-100 (score: 0.3712840676307678).\n{'train_runtime': 301.8995, 'train_samples_per_second': 15.899, 'train_steps_per_second': 0.994, 'train_loss': 0.2194733174641927, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [05:01<00:00,  1.01s/it]\nSome weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  4.17ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.66ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.43ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.38ba/s]\nTrain data statistics: {'total': 1600, 'positive': 793, 'negative': 807}\nTest data statistics: {'total': 400, 'positive': 207, 'negative': 193}\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars. If language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\n  Number of trainable parameters = 110618882\n  0%|                                                   | 0/300 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 33%|█████████████▋                           | 100/300 [01:30<02:51,  1.16it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars. If language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.17it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.12it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.67it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.44it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.24it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.16it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.11it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.06it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:04,  3.03it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  3.02it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  3.01it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.99it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  3.00it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.99it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.99it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.97it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.98it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.96it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.95it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.97it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.96it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.95it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.26745083928108215, 'eval_accuracy': 0.9, 'eval_precision': 0.9562841530054644, 'eval_recall': 0.8454106280193237, 'eval_f1': 0.8974358974358975, 'eval_runtime': 8.3976, 'eval_samples_per_second': 47.633, 'eval_steps_per_second': 2.977, 'epoch': 1.0}\n 33%|█████████████▋                           | 100/300 [01:38<02:51,  1.16it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.96it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-100\nConfiguration saved in output/checkpoint-100/config.json\nModel weights saved in output/checkpoint-100/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 67%|███████████████████████████▎             | 200/300 [03:09<01:26,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars. If language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.12it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.07it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.61it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:05,  3.39it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.20it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.10it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.06it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.03it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  3.00it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.98it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.97it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.97it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.96it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.95it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.96it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.96it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.93it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.92it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.91it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.91it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.92it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.92it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.93it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2674526572227478, 'eval_accuracy': 0.9125, 'eval_precision': 0.9432989690721649, 'eval_recall': 0.8840579710144928, 'eval_f1': 0.9127182044887779, 'eval_runtime': 8.4963, 'eval_samples_per_second': 47.079, 'eval_steps_per_second': 2.942, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 200/300 [03:18<01:26,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.95it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-200\nConfiguration saved in output/checkpoint-200/config.json\nModel weights saved in output/checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 300/300 [04:49<00:00,  1.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars. If language, review_title, product_id, review_body, review_id, product_category, reviewer_id, stars are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 400\n  Batch size = 16\n\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.11it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:05,  4.07it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:05,  3.58it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:06,  3.32it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:01<00:06,  3.16it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:05,  3.06it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:05,  3.03it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:02<00:05,  3.01it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.98it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:04,  2.98it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:03<00:04,  2.99it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.98it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:04<00:03,  2.99it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  2.97it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.98it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:05<00:02,  2.97it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  2.96it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.98it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:06<00:01,  2.96it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  2.96it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:07<00:01,  2.97it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:07<00:00,  2.96it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.38514724373817444, 'eval_accuracy': 0.905, 'eval_precision': 0.9082125603864735, 'eval_recall': 0.9082125603864735, 'eval_f1': 0.9082125603864735, 'eval_runtime': 8.4597, 'eval_samples_per_second': 47.283, 'eval_steps_per_second': 2.955, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [04:57<00:00,  1.15it/s]\n100%|███████████████████████████████████████████| 25/25 [00:08<00:00,  2.97it/s]\u001b[A\n                                                                                \u001b[ASaving model checkpoint to output/checkpoint-300\nConfiguration saved in output/checkpoint-300/config.json\nModel weights saved in output/checkpoint-300/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-100 (score: 0.26745083928108215).\n{'train_runtime': 301.8473, 'train_samples_per_second': 15.902, 'train_steps_per_second': 0.994, 'train_loss': 0.23457532246907553, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 300/300 [05:01<00:00,  1.01s/it]\n5min 21s ± 2.25 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}